import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers

DATA_DIR = '../data/'
EMBED_SIZE = 128
MAX_FEATURES = 2000
MAX_SENTENCE_LEN = 200
OUTPUT_SIZE = 60

# read dataset
train_f = pd.read_csv(DATA_DIR+'train.csv')
test_f = pd.read_csv(DATA_DIR+'test.csv')

# replace null values with empty string
train_f.fillna('', inplace=True)
test_f.fillna('', inplace=True)

#print (len(train_f))
#print (train_f.head())
#print (train_f.isnull().any(),test_f.isnull().any())

# extract labels and sentences(texts).
list_classes = ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]
train_labels = train_f[list_classes].values

train_texts = train_f["comment_text"]
test_texts = test_f["comment_text"]

# tokenize each word to an index
tokenizer = Tokenizer(num_words=MAX_FEATURES)
tokenizer.fit_on_texts(list(train_texts))

# tokenizer.word_counts: dictionary of <word, count>
# tokenizer.word_index: dictionary of <word, index>

# convert sentences into lists of indexes
# train_tokens is a list of lists,
# and each nested list corresponds to a sentence.
train_tokens = tokenizer.texts_to_sequences(train_texts)
test_tokens = tokenizer.texts_to_sequences(test_texts)

# pad training & testing data to make each sentence in the same length
train_tokens = pad_sequences(train_tokens,maxlen=MAX_SENTENCE_LEN)
test_tokens = pad_sequences(test_tokens,maxlen=MAX_SENTENCE_LEN)

input_layer = Input(shape=(MAX_SENTENCE_LEN, ))
embed_layer = Embedding(MAX_FEATURES, EMBED_SIZE)(input_layer)
# 60 is the outputsize
rnn_layer = LSTM(60, return_sequences=True,name='lstm_layer')(embed_layer)
max_pooling_layer = GlobalMaxPool1D()(rnn_layer)
# randomly dropout neurons with 0.1 probability
drop_out_layer1 = Dropout(0.1)(max_pooling_layer)
dense_layer = Dense(50, activation="relu")(drop_out_layer1)
drop_out_layer2 = Dropout(0.1)(dense_layer)
output_layer = Dense(6, activation="sigmoid")(drop_out_layer2)

model = Model(inputs=input_layer, outputs=output_layer)
model.compile(
        loss='binary_crossentropy',
        optimizer='adam',
        metrics=['accuracy'])

batch_size = 32
epochs = 2
model.fit(
        train_tokens,
        train_labels,
        batch_size=batch_size,
        epochs=epochs,
        validation_split=0.1)

#model.summary()
